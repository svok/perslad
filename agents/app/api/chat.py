import json
import logging
import time
from typing import List, Dict, Any
from fastapi.responses import StreamingResponse
from langchain_core.messages import HumanMessage, AIMessage

from ..config import Config
from ..core.graph import create_graph
from ..managers.system import SystemManager
from ..core.utils import estimate_tokens

logger = logging.getLogger("agentnet.chat")

class ChatHandler:
    def __init__(self, system: SystemManager):
        self.system = system
        self._graph = None

    def _convert_messages(self, messages: List[Dict]) -> List:
        converted = []
        for msg in messages:
            if msg["role"] == "user":
                converted.append(HumanMessage(content=msg["content"]))
            elif msg["role"] == "assistant":
                converted.append(AIMessage(content=msg["content"]))
        return converted

    async def _ensure_graph(self):
        if self._graph is None:
            if not self.system.llm.is_ready():
                logger.error("Cannot create graph: LLM not connected")
                return None

            model = self.system.llm.get_model()
            raw_tools = await self.system.tools.get_tools()

            if model and raw_tools:
                logger.info(f"üèóÔ∏è  Creating graph with {len(raw_tools)} tools")
                logger.info(f"üîß Tools: {[t['name'] for t in raw_tools]}")

                formatted_tools = []
                for t in raw_tools:
                    schema = t.get("inputSchema", {"type": "object", "properties": {}})
                    if not schema.get("properties"):
                        schema["additionalProperties"] = True
                    formatted_tools.append({
                        "type": "function",
                        "function": {
                            "name": t["name"],
                            "description": t.get("description", ""),
                            "parameters": schema
                        }
                    })

                logger.info(f"Formatted Tools: {formatted_tools}")
                model_with_tools = model.bind_tools(formatted_tools, tool_choice="auto")

                self._graph = create_graph(model_with_tools, self.system.tools)
                logger.info("‚úÖ Graph created (Native LangChain + vLLM Qwen Parser)")
            else:
                logger.error("‚ùå Cannot create graph")
                return None

        return self._graph

    async def direct_response(self, messages: List[Dict]) -> Dict[str, Any]:
        system_status = self.system.get_status()
        if not system_status['llm_ready']:
            return {"error": {"message": "LLM connection lost"}}

        graph = await self._ensure_graph()
        if not graph:
            return {"error": {"message": "System initialization failed"}}

        try:
            # 1. –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è
            converted_messages = self._convert_messages(messages)

            # === –£–ü–†–ê–í–õ–ï–ù–ò–ï –ö–û–ù–¢–ï–ö–°–¢–û–ú ===

            # 2. –°—á–∏—Ç–∞–µ–º —Å—Ç–æ–∏–º–æ—Å—Ç—å –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
            # –ü–æ–ª—É—á–∞–µ–º –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Ä–∞—Å—á–µ—Ç–∞ –∏—Ö —Ä–∞–∑–º–µ—Ä–∞
            raw_tools = await self.system.tools.get_tools()
            # –°–µ—Ä–∏–∞–ª–∏–∑—É–µ–º –≤ —Å—Ç—Ä–æ–∫—É, —á—Ç–æ–±—ã –æ—Ü–µ–Ω–∏—Ç—å —Ä–∞–∑–º–µ—Ä –≤ —Ç–æ–∫–µ–Ω–∞—Ö
            tools_json_str = json.dumps(raw_tools, ensure_ascii=False)
            tools_tokens = estimate_tokens(tools_json_str)

            # 3. –°—á–∏—Ç–∞–µ–º —Å—Ç–æ–∏–º–æ—Å—Ç—å –ò—Å—Ç–æ—Ä–∏–∏
            history_tokens = sum(
                estimate_tokens(msg.content) for msg in converted_messages
            )
            system_tokens = estimate_tokens(Config.SYSTEM_PROMPT)
            if system_tokens + tools_tokens >= Config.MAX_MODEL_TOKENS - Config.SAFETY_MARGIN:
                logger.error(
                    f"üö® System ({system_tokens}) + tools ({tools_tokens}) exceed "
                    f"context window {Config.MAX_MODEL_TOKENS}"
                )
                return {
                    "error": {
                        "message": "System prompt and tools exceed model context window"
                    }
                }



    # 4. –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            total_used = system_tokens + tools_tokens + history_tokens
            limit_threshold = Config.MAX_MODEL_TOKENS - Config.SAFETY_MARGIN

            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤ (–¢—Ä–µ–±–æ–≤–∞–Ω–∏–µ –∞)
            logger.info(
                f"üìä Context Usage: "
                f"System={system_tokens}, "
                f"Tools={tools_tokens}, "
                f"History={history_tokens} | "
                f"Total={total_used}/{Config.MAX_MODEL_TOKENS} (Limit={limit_threshold})"
            )

            final_messages = converted_messages

            # 5. –ö–æ–Ω—Ç—Ä–æ–ª—å –∏ –æ–±—Ä–µ–∑–∫–∞ (–¢—Ä–µ–±–æ–≤–∞–Ω–∏–µ –±)
            if total_used > limit_threshold:
                logger.warning(f"‚ö†Ô∏è Context Overflow! Truncating history to fit {Config.MAX_MODEL_TOKENS} limit...")

                # –í—ã—á–∏—Å–ª—è–µ–º –±—é–¥–∂–µ—Ç –¥–ª—è –∏—Å—Ç–æ—Ä–∏–∏ (–í—Å–µ–≥–æ - –°–∏—Å—Ç–µ–º–∞ - –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã - –ó–∞–ø–∞—Å)
                available_for_history = (
                        Config.MAX_MODEL_TOKENS
                        - system_tokens
                        - tools_tokens
                        - Config.SAFETY_MARGIN
                )

                if available_for_history < 0:
                    logger.error("üö® Tools and System prompt exceed max context! Request will fail.")
                else:
                    # –û–±—Ä–µ–∑–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é —Å –Ω–∞—á–∞–ª–∞, –ø–æ–∫–∞ –Ω–µ –≤–ª–µ–∑–µ–º –≤ –±—é–¥–∂–µ—Ç
                    # –°–∫–æ–ª—å–∑—è—â–µ–µ –æ–∫–Ω–æ: —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è
                    current_size = 0
                    truncated_list = []

                    for msg in reversed(converted_messages):
                        msg_size = estimate_tokens(msg.content)

                        # –æ–¥–∏–Ω–æ—á–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –±–æ–ª—å—à–µ –±—é–¥–∂–µ—Ç–∞ ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                        if msg_size > available_for_history:
                            continue

                        if current_size + msg_size > available_for_history:
                            break

                        truncated_list.insert(0, msg)
                        current_size += msg_size

                    final_messages = truncated_list
                    logger.info(f"‚úÇÔ∏è History truncated from {len(converted_messages)} to {len(final_messages)} messages.")

            # –ó–∞–ø—É—Å–∫ –≥—Ä–∞—Ñ–∞ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —Å–ø–∏—Å–∫–æ–º
            initial_state = {
                "messages": final_messages
            }

            result = await graph.ainvoke(initial_state)
            final_messages_res = result["messages"]

            last_ai_message = None
            for msg in reversed(final_messages_res):
                if isinstance(msg, AIMessage) and not msg.tool_calls:
                    last_ai_message = msg
                    break

            content = ""
            if last_ai_message:
                content = last_ai_message.content
            elif final_messages_res and isinstance(final_messages_res[-1], AIMessage):
                content = final_messages_res[-1].content or "Action executed."
            else:
                content = "No response generated."

            return {
                "id": f"chatcmpl-{int(time.time())}",
                "object": "chat.completion",
                "created": int(time.time()),
                "choices": [{
                    "message": {
                        "role": "assistant",
                        "content": content
                    },
                    "finish_reason": "stop"
                }]
            }

        except Exception as e:
            logger.error(f"‚ùå Graph execution error: {e}", exc_info=True)
            return {"error": {"message": str(e)}}

    async def stream_response(self, messages: List[Dict]) -> StreamingResponse:
        async def fake_stream():
            result = await self.direct_response(messages)
            import json
            if "error" in result:
                yield f"data: {json.dumps(result)}\n\n"
            else:
                choice = result["choices"][0]
                yield f"data: {json.dumps({'choices': [{'delta': {'role': 'assistant'}}]})}\n\n"
                yield f"data: {json.dumps({'choices': [{'delta': {'content': choice['message']['content']}}]})}\n\n"
                yield f"data: {json.dumps({'choices': [{'finish_reason': 'stop'}]})}\n\n"
            yield "data: [DONE]\n\n"

        return StreamingResponse(fake_stream(), media_type="text/event-stream")
